# æ¹¾åŒºç¾Žé£Ÿåœ°å›¾ â€” Engineering Roadmap

> Last updated: 2026-02-19 (v3 â€” P0+P1+P2 implemented)
> Status: Pipeline fully operational. Frontend split. Review workflow. Google enrichment ready.

---

## Current State (as of 2026-02-19)

### Data
| Metric | Value |
|--------|-------|
| Total restaurants | 79 |
| With dishes (recommendations) | 43 (54%) |
| With Google Maps data | 78 (99%) |
| Google-verified | 8 (10%) |
| Regions | South Bay 55 Â· East Bay 18 Â· SF 3 Â· Peninsula 3 |
| DB size | ~80KB JSON |

### Architecture
```
XHS MCP â†’ pipeline/01_scrape.sh
         â†’ pipeline/02_extract.js   (regex-based candidate extraction)
         â†’ pipeline/03_merge.js     (safe append-only merge)
         â†’ pipeline/04_verify.js    (integrity check + auto-restore)
         â†’ data/restaurant_database.json
                â†“
         index.html (vanilla JS, fetches full JSON on load)
```

### What works
- âœ… Daily cron at 11:00 AM (fixed â€” was silently failing for days)
- âœ… Auto-backup before every run, auto-restore on verify failure
- âœ… `data/corrections.json` â€” manual corrections survive pipeline runs
- âœ… Clean git history, single source of truth

### Known Weaknesses
See P0/P1 issues below.

---

## Issues & Improvements (Prioritized)

### âœ… P0 â€” DONE (2026-02-19)

#### 1. XHS Login Expiry Notification âœ…
**Fixed:** `run.sh` now sends an `openclaw system event` alert when the XHS scrape is skipped due to auth failure.
Touch sentinel `.scrape_complete` when scrape succeeds; alert if absent after scrape step.

#### 2. LLM-Based Restaurant Extraction âœ…
**Fixed:** Replaced fragile regex `02_extract.js` with `02_extract_llm.js` using Gemini 1.5 Flash.
- Extracts restaurant name, city, cuisine, dishes, sentiment in one LLM pass
- Rate-limited: max 30 posts/run, 500ms between API calls
- Graceful fallback: parse errors â†’ empty array (pipeline continues)
- Requires `GEMINI_API_KEY` in `.env`

---

### âœ… P1 â€” DONE (2026-02-19)

#### 3. Engagement Metric Updates âœ…
**Implemented:** `03_update_metrics.js` â€” runs before merge step.
- Matches new post candidates against existing restaurants by normalized name
- Increments `mention_count`, `total_engagement`, appends to `sources[]`
- Maintains `trend_30d` (rolling 90-day window of `{date, count, engagement}`)
- Appends new LLM-extracted dishes to `recommendations[]` (deduped)
- Weighted sentiment update (10% weight to new signal)

#### 4. Pipeline State & Monitoring âœ…
**Implemented:** `run.sh` writes `data/.pipeline_state.json` after every run:
```json
{
  "last_run": "2026-02-19T15:51:26Z",
  "status": "success",
  "restaurants_total": 79,
  "restaurants_added": 0,
  "restaurants_metrics_updated": 0,
  "posts_scraped": 0,
  "scrape_ok": true,
  "dry_run": false
}
```
Frontend now reads this and displays "æ›´æ–°äºŽ XæœˆYæ—¥ HH:MM" in the header subtitle.

**Also done as part of P1:**
- `config.sh` â€” centralised paths/env, sourced by `run.sh` (no more hardcoded paths)
- `06_generate_index.js` â€” slim 33KB index for fast initial page load (74% smaller than full DB)
- `index.html` loads slim index first, falls back to full DB; lazy-loads pipeline state for timestamp

---

### âœ… P2 â€” DONE (2026-02-19)

#### 6. Candidate Review Workflow âœ…
**Implemented:** `pipeline/review.js` â€” interactive CLI for reviewing new candidates before they enter the main DB.
- Keys: [y] approve / [n] reject / [s] skip / [q] quit & save
- Approved candidates written to `data/candidates/approved/YYYY-MM-DD.json`
- `--auto-approve` flag for CI/unattended mode
- `--date YYYY-MM-DD` to review a specific day's candidates

#### 7. Frontend: Component Extraction âœ…
**Implemented:**
- `index.html` reduced from 517 â†’ ~90 lines (pure HTML template)
- All JS extracted to `src/app.js` (no build step, plain `<script src>`)
- All CSS extracted to `src/styles.css` (plain `<link rel="stylesheet">`)

#### 8. Auto Git Commit After Pipeline âœ…
**Implemented:** `pipeline/07_commit.sh` â€” called by `run.sh` after each successful run.
- Only commits if tracked data files actually changed (idempotent)
- Commit message: `data: YYYY-MM-DD pipeline +N restaurants (total: X)`

#### 9. Google Places Enrichment âœ… (P3 pulled forward)
**Implemented:** `pipeline/enrich_google.js`
- Searches Google Places API for each unverified restaurant
- Picks best match by Levenshtein name similarity (threshold: 40%)
- Populates: `google_place_id`, `address`, `google_rating`, `lat/lng`, `verified=true`
- Usage: `node pipeline/enrich_google.js --limit 20`
- Requires: `GOOGLE_PLACES_API_KEY` in `.env`
- Cost: ~$0.017/restaurant (Text Search + Place Details)

---

### ðŸ”µ P3 â€” Longer Term

#### 10. Google Places Verification âœ… (done in P2)

#### 11. Timeseries Tracking âœ…
**Implemented:** Replaced `trend_30d` (scalar) with `timeseries` (monthly array):
```json
"timeseries": [
  {"month": "2026-01", "mentions": 3, "engagement": 45},
  {"month": "2026-02", "mentions": 7, "engagement": 112}
]
```
- `03_update_metrics.js`: writes monthly entries, keeps last 24 months; inline migration resets any legacy scalar to `[]`
- `04_merge.js`: new restaurants start with `timeseries: []`
- `06_generate_index.js`: includes last 12 months in slim index
- `src/app.js` `generateChart()`: uses `timeseries` directly if present; falls back to `post_details` aggregation

#### 12. Structured Error Recovery for XHS Auth âœ…
**Implemented:**
- `01_scrape.sh`: exits with code `2` (not `0`) on auth failure so `run.sh` correctly sets `SCRAPE_OK=false`
- `run.sh` `write_state()`: adds `"last_scrape_at"` field â€” updated only on successful scrapes, preserved otherwise
- `run.sh` startup health check: reads previous `last_scrape_at`; if >3 days old, fires a recurring notification every run until auth is restored

---

## Current Pipeline (as of 2026-02-19)

```
run.sh (cron entry)
  â”œâ”€â”€ 01_scrape.sh          â†’ data/raw/YYYY-MM-DD/post_*.json  (XHS MCP)
  â”œâ”€â”€ 02_extract_llm.js     â†’ data/candidates/YYYY-MM-DD.json  (Gemini LLM)
  â”œâ”€â”€ 03_update_metrics.js  â†’ IN-PLACE: update metrics for existing restaurants
  â”œâ”€â”€ 04_merge.js           â†’ data/restaurant_database.json    (append-only)
  â”œâ”€â”€ scripts/apply_corrections.js â†’ apply data/corrections.json
  â”œâ”€â”€ 05_verify.js          â†’ integrity check, auto-restore on fail
  â”œâ”€â”€ 06_generate_index.js  â†’ data/restaurant_database_index.json (slim, 74% smaller)
  â”œâ”€â”€ write .pipeline_state.json
  â”œâ”€â”€ 07_commit.sh          â†’ git commit "data: YYYY-MM-DD +N restaurants"
  â””â”€â”€ notify on auth failure or new restaurants
```

Each step is independent, idempotent, and can be run in isolation.

---

## Current Directory Structure âœ…

```
bay-area-food-map/
â”œâ”€â”€ .env                          # API keys (gitignored)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ ROADMAP.md
â”œâ”€â”€ config.sh                     # Centralised paths + env (sourced by pipeline)
â”œâ”€â”€ dev.js                        # Local dev server â†’ http://localhost:8080
â”œâ”€â”€ index.html                    # Frontend (single-page, no build step)
â”œâ”€â”€ package.json                  # 4 scripts: dev / pipeline / pipeline:dry / test
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ restaurant_database.json       # Source of truth (git tracked)
â”‚   â”œâ”€â”€ restaurant_database_index.json # Slim 33KB for fast page load (git tracked)
â”‚   â”œâ”€â”€ corrections.json               # Manual corrections (git tracked)
â”‚   â”œâ”€â”€ .pipeline_state.json           # Written by run.sh after each run
â”‚   â”œâ”€â”€ raw/                           # Daily scraped posts (gitignored)
â”‚   â”œâ”€â”€ candidates/                    # LLM extraction output (gitignored)
â”‚   â””â”€â”€ backups/                       # Auto-backups, 7-day TTL (gitignored)
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ run.sh                    # Orchestrator â€” cron calls this
â”‚   â”œâ”€â”€ 01_scrape.sh              # XHS MCP â†’ data/raw/YYYY-MM-DD/
â”‚   â”œâ”€â”€ 02_extract_llm.js         # Gemini LLM â†’ data/candidates/YYYY-MM-DD.json
â”‚   â”œâ”€â”€ 03_update_metrics.js      # Update engagement/trend for existing restaurants
â”‚   â”œâ”€â”€ 04_merge.js               # Append-only merge into restaurant_database.json
â”‚   â”œâ”€â”€ 05_verify.js              # Integrity check + auto-restore
â”‚   â””â”€â”€ 06_generate_index.js      # Regenerate slim index
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ apply_corrections.js      # Apply data/corrections.json (called by run.sh)
â”‚   â””â”€â”€ transaction.js            # Atomic write + rollback helper
â”‚
â””â”€â”€ tests/
    â””â”€â”€ verify.js                 # 22-check functional test suite (no deps, <10s)
```

---

## What NOT to Build (Anti-patterns)

- **No backend API** â€” The data is static enough that a JSON file served by nginx is simpler and faster than an API server. Add an API only when you need user-generated content or real-time data.
- **No database (SQLite/Postgres)** â€” 79â€“500 restaurants in a JSON file is fast, auditable, and versionable with git. A database adds operational overhead without benefit at this scale.
- **No React/Vue/etc** â€” The app is a filtered list with modals. Vanilla JS handles this fine. A framework would require a build step, CI, and ongoing dependency updates.
- **No Docker** â€” This runs on a single Mac. Docker adds complexity without benefit here.
- **No microservices** â€” Same reasoning. The pipeline is a simple sequential bash script.

---

## Next Session: Where to Start

P0 and P1 are done. Pick up from **P2**:

1. **Candidate review workflow** (2 hrs) â€” `pipeline/review.js` interactive CLI; new restaurants queue in `data/candidates/` before going live
2. **Auto git commit after pipeline** (30 min) â€” `07_commit.sh`: `git add data/ && git commit -m "data: YYYY-MM-DD +N restaurants"`
3. **Frontend component split** (2-3 hrs) â€” extract inline JS to `src/app.js`, CSS to `src/styles.css`
4. **Google Places enrichment for new restaurants** (P3) â€” `pipeline/enrich_google.js` using existing `GOOGLE_PLACES_API_KEY`

**Before any of the above:** Test a real pipeline run with live XHS data (not dry-run) to validate the full end-to-end LLM extraction flow.

---

*Written with the perspective of: senior software engineer (architecture), data engineer (pipeline design), frontend engineer (UI/UX scalability).*
