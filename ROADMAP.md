# æ¹¾åŒºç¾Žé£Ÿåœ°å›¾ â€” Engineering Roadmap

> Last updated: 2026-02-22 (v5 â€” Pipeline redesigned with Google Place ID matching)
> Status: Pipeline fully operational. Google enrichment integrated. No more manual review required.

---

## Bug Fixes

- **2026-02-19** â€” Fixed `è®¨è®ºåº¦` always showing 0: `src/app.js` was reading `r.engagement` but the database stores this as `r.total_engagement`. Field name corrected in sort, card view, and modal (3 occurrences).

## Current State (as of 2026-02-19)

### Data
| Metric | Value |
|--------|-------|
| Total restaurants | 79 |
| With dishes (recommendations) | 43 (54%) |
| With Google Maps data | 78 (99%) |
| Google-verified | 8 (10%) |
| Regions | South Bay 55 Â· East Bay 18 Â· SF 3 Â· Peninsula 3 |
| DB size | ~80KB JSON |

### Architecture
```
XHS MCP â†’ pipeline/01_scrape.sh
         â†’ pipeline/02_extract.js   (regex-based candidate extraction)
         â†’ pipeline/03_merge.js     (safe append-only merge)
         â†’ pipeline/04_verify.js    (integrity check + auto-restore)
         â†’ data/restaurant_database.json
                â†“
         index.html (vanilla JS, fetches full JSON on load)
```

### What works
- âœ… Daily cron at 11:00 AM (fixed â€” was silently failing for days)
- âœ… Auto-backup before every run, auto-restore on verify failure
- âœ… `data/corrections.json` â€” manual corrections survive pipeline runs
- âœ… Clean git history, single source of truth

### Known Weaknesses
See P0/P1 issues below.

---

## Issues & Improvements (Prioritized)

### âœ… P0 â€” DONE (2026-02-19)

#### 1. XHS Login Expiry Notification âœ…
**Fixed:** `run.sh` now sends an `openclaw system event` alert when the XHS scrape is skipped due to auth failure.
Touch sentinel `.scrape_complete` when scrape succeeds; alert if absent after scrape step.

#### 2. LLM-Based Restaurant Extraction âœ…
**Fixed:** Replaced fragile regex `02_extract.js` with `02_extract_llm.js` using Kimi K2.5 (Kimi Code API).
- Extracts restaurant name, city, cuisine, dishes, sentiment in one LLM pass
- Rate-limited: max 30 posts/run, 500ms between API calls
- Graceful fallback: parse errors â†’ empty array (pipeline continues)
- Requires `KIMI_API_KEY` in `.env`

---

### âœ… P1 â€” DONE (2026-02-19)

#### 3. Engagement Metric Updates âœ…
**Implemented:** `03_update_metrics.js` â€” runs before merge step.
- Matches new post candidates against existing restaurants by normalized name
- Increments `mention_count`, `total_engagement`, appends to `sources[]`
- Maintains `trend_30d` (rolling 90-day window of `{date, count, engagement}`)
- Appends new LLM-extracted dishes to `recommendations[]` (deduped)
- Weighted sentiment update (10% weight to new signal)

#### 4. Pipeline State & Monitoring âœ…
**Implemented:** `run.sh` writes `data/.pipeline_state.json` after every run:
```json
{
  "last_run": "2026-02-19T15:51:26Z",
  "status": "success",
  "restaurants_total": 79,
  "restaurants_added": 0,
  "restaurants_metrics_updated": 0,
  "posts_scraped": 0,
  "scrape_ok": true,
  "dry_run": false
}
```
Frontend now reads this and displays "æ›´æ–°äºŽ XæœˆYæ—¥ HH:MM" in the header subtitle.

**Also done as part of P1:**
- `config.sh` â€” centralised paths/env, sourced by `run.sh` (no more hardcoded paths)
- `06_generate_index.js` â€” slim 33KB index for fast initial page load (74% smaller than full DB)
- `index.html` loads slim index first, falls back to full DB; lazy-loads pipeline state for timestamp

---

### âœ… P2 â€” DONE (2026-02-19)

#### 6. Candidate Review Workflow âœ…
**Implemented:** `pipeline/review.js` â€” interactive CLI for reviewing new candidates before they enter the main DB.
- Keys: [y] approve / [n] reject / [s] skip / [q] quit & save
- Approved candidates written to `data/candidates/approved/YYYY-MM-DD.json`
- `--auto-approve` flag for CI/unattended mode
- `--date YYYY-MM-DD` to review a specific day's candidates

#### 7. Frontend: Component Extraction âœ…
**Implemented:**
- `index.html` reduced from 517 â†’ ~90 lines (pure HTML template)
- All JS extracted to `src/app.js` (no build step, plain `<script src>`)
- All CSS extracted to `src/styles.css` (plain `<link rel="stylesheet">`)

#### 8. Auto Git Commit After Pipeline âœ…
**Implemented:** `pipeline/07_commit.sh` â€” called by `run.sh` after each successful run.
- Only commits if tracked data files actually changed (idempotent)
- Commit message: `data: YYYY-MM-DD pipeline +N restaurants (total: X)`

#### 9. Google Places Enrichment âœ… (P3 pulled forward)
**Implemented:** `pipeline/enrich_google.js`
- Searches Google Places API for each unverified restaurant
- Picks best match by Levenshtein name similarity (threshold: 40%)
- Populates: `google_place_id`, `address`, `google_rating`, `lat/lng`, `verified=true`
- Usage: `node pipeline/enrich_google.js --limit 20`
- Requires: `GOOGLE_PLACES_API_KEY` in `.env`
- Cost: ~$0.017/restaurant (Text Search + Place Details)

---

### ðŸ”µ P3 â€” Longer Term

#### 10. Google Places Verification âœ… (done in P2)

#### 11. Timeseries Tracking âœ…
**Implemented:** Replaced `trend_30d` (scalar) with `timeseries` (monthly array):
```json
"timeseries": [
  {"month": "2026-01", "mentions": 3, "engagement": 45},
  {"month": "2026-02", "mentions": 7, "engagement": 112}
]
```
- `03_update_metrics.js`: writes monthly entries, keeps last 24 months; inline migration resets any legacy scalar to `[]`
- `04_merge.js`: new restaurants start with `timeseries: []`
- `06_generate_index.js`: includes last 12 months in slim index
- `src/app.js` `generateChart()`: uses `timeseries` directly if present; falls back to `post_details` aggregation

#### 12. Structured Error Recovery for XHS Auth âœ…
**Implemented:**
- `01_scrape.sh`: exits with code `2` (not `0`) on auth failure so `run.sh` correctly sets `SCRAPE_OK=false`
- `run.sh` `write_state()`: adds `"last_scrape_at"` field â€” updated only on successful scrapes, preserved otherwise
- `run.sh` startup health check: reads previous `last_scrape_at`; if >3 days old, fires a recurring notification every run until auth is restored

---

## Current Pipeline (as of 2026-02-22)

```
run.sh (cron entry)
  â”œâ”€â”€ 01_scrape.sh          â†’ data/raw/YYYY-MM-DD/post_*.json  (XHS MCP)
  â”œâ”€â”€ 02_extract_llm.js     â†’ data/candidates/YYYY-MM-DD.json  (GLM-5 LLM)
  â”œâ”€â”€ 03_enrich_candidates.js â†’ enrich candidates with Google Places data
  â”œâ”€â”€ 04_merge.js           â†’ data/restaurant_database.json    (merge by place_id)
  â”œâ”€â”€ 05_verify.js          â†’ integrity check, auto-restore on fail
  â”œâ”€â”€ 06_generate_index.js  â†’ data/restaurant_database_index.json (slim, 74% smaller)
  â”œâ”€â”€ write .pipeline_state.json
  â”œâ”€â”€ 07_commit.sh          â†’ git commit "data: YYYY-MM-DD +N restaurants"
  â””â”€â”€ notify on auth failure or new restaurants
```

Each step is independent, idempotent, and can be run in isolation.

---

## Current Directory Structure âœ…

```
bay-area-food-map/
â”œâ”€â”€ .env                          # API keys (gitignored)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ ROADMAP.md
â”œâ”€â”€ config.sh                     # Centralised paths + env (sourced by pipeline)
â”œâ”€â”€ dev.js                        # Local dev server â†’ http://localhost:8080
â”œâ”€â”€ index.html                    # Frontend (single-page, no build step)
â”œâ”€â”€ package.json                  # scripts: dev / pipeline / pipeline:dry / test / test:e2e
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ restaurant_database.json       # Source of truth (git tracked)
â”‚   â”œâ”€â”€ restaurant_database_index.json # Slim 33KB for fast page load (git tracked)
â”‚   â”œâ”€â”€ corrections.json               # Manual corrections (git tracked)
â”‚   â”œâ”€â”€ .pipeline_state.json           # Written by run.sh after each run
â”‚   â”œâ”€â”€ raw/                           # Daily scraped posts (gitignored)
â”‚   â”œâ”€â”€ candidates/                    # LLM extraction output (gitignored)
â”‚   â””â”€â”€ backups/                       # Auto-backups, 7-day TTL (gitignored)
â”‚
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ run.sh                    # Orchestrator â€” cron calls this
â”‚   â”œâ”€â”€ 01_scrape.sh              # XHS MCP â†’ data/raw/YYYY-MM-DD/
â”‚   â”œâ”€â”€ 02_extract_llm.js         # GLM-5 LLM â†’ data/candidates/YYYY-MM-DD.json
â”‚   â”œâ”€â”€ 03_enrich_candidates.js   # Google Places enrichment for candidates
â”‚   â”œâ”€â”€ 04_merge.js               # Merge by place_id into restaurant_database.json
â”‚   â”œâ”€â”€ 05_verify.js              # Integrity check + auto-restore
â”‚   â””â”€â”€ 06_generate_index.js      # Regenerate slim index
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ apply_corrections.js      # Manual use only â€” apply data/corrections.json
â”‚   â””â”€â”€ transaction.js            # Atomic write + rollback helper
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ verify.js                 # 22-check functional test suite (no deps, <10s)
    â””â”€â”€ e2e.js                    # Full end-to-end integration test (real XHS + Kimi LLM)
```

---

### âœ… P4 â€” DONE (2026-02-21)

#### 13. Switch LLM to Kimi K2.5 âœ…
**Implemented:** `pipeline/02_extract_llm.js` migrated from Gemini to Kimi Code API.
- Endpoint: `https://api.kimi.com/coding/v1/chat/completions`, model `kimi-for-coding`
- OpenAI-compatible request format; `User-Agent: KimiCLI/1.3` header required
- Requires `KIMI_API_KEY` in `.env`

#### 14. XHS MCP JSON-RPC Envelope Fix âœ…
**Fixed:** All three MCP tool calls in `01_scrape.sh` updated to unwrap the JSON-RPC envelope (`result.content[0].text`) before parsing:
- `check_login_status`: now correctly detects `å·²ç™»å½•`
- `search_feeds`: now reads `interactInfo` from inside `noteCard`; handles string counts
- `get_feed_detail`: param renamed `note_id` â†’ `feed_id`; response normalized from `data.note.*` to flat structure

#### 15. Merge Field Propagation Fix âœ…
**Fixed:** `pipeline/04_merge.js` was hardcoding `city/cuisine/price_range: 'unknown'` instead of using candidate values. Now correctly propagates all candidate fields into the merged restaurant.

#### 16. End-to-End Integration Test âœ…
**Implemented:** `tests/e2e.js` â€” full live pipeline test, no mocks.
- 7 phases: setup â†’ real XHS scrape â†’ real Kimi extraction â†’ merge â†’ verify â†’ approve + index â†’ frontend shape
- `E2E_QUICK=1` mode skips scrape, uses baked-in sample post for fast local dev testing
- Sample post named `post_000_test_001.json` to sort before all scraped hex-ID posts
- Extraction capped at 10 posts with 420s timeout (worst case: 10 Ã— 30s Kimi timeout)
- Run: `node tests/e2e.js` or `npm run test:e2e`

---

### âœ… P5 â€” DONE (2026-02-22)

#### 17. Pipeline Redesign with Google Place ID Matching âœ…
**Implemented:** Complete pipeline redesign to use Google Place ID as the unique identifier for deduplication.

**Changes:**
- **NEW:** `pipeline/03_enrich_candidates.js` â€” Enriches LLM candidates with Google Places data before merging
- **MODIFIED:** `pipeline/04_merge.js` â€” Now uses `google_place_id` for matching instead of name-based matching
  - Updates existing restaurants when place_id matches (metrics, engagement, dishes)
  - Adds new restaurants with `verified: true`, `needs_review: false`
- **DELETED:** `pipeline/03_update_metrics.js` â€” Functionality absorbed into new merge script
- **REMOVED:** `apply_corrections.js` step from pipeline (kept for manual use only)

**Benefits:**
- No more duplicates from bilingual name variations (e.g., "é¦™å°é¦† Shang Cafe" vs "é¦™å°é¦†")
- No manual review required â€” all merged restaurants are Google-verified
- CJK-aware matching: trusts Google's top result if city matches
- Non-CJK matching: requires â‰¥40% name similarity

**Cost:** ~$0.017 per candidate (Text Search + Place Details)

---

## What NOT to Build (Anti-patterns)

- **No backend API** â€” The data is static enough that a JSON file served by nginx is simpler and faster than an API server. Add an API only when you need user-generated content or real-time data.
- **No database (SQLite/Postgres)** â€” 79â€“500 restaurants in a JSON file is fast, auditable, and versionable with git. A database adds operational overhead without benefit at this scale.
- **No React/Vue/etc** â€” The app is a filtered list with modals. Vanilla JS handles this fine. A framework would require a build step, CI, and ongoing dependency updates.
- **No Docker** â€” This runs on a single Mac. Docker adds complexity without benefit here.

---

## Next Session: Where to Start

P0â€“P5 are done. The pipeline is fully operational with Google Place ID-based deduplication. Potential next improvements:

1. **Backfill existing unverified restaurants** â€” Run `node pipeline/enrich_google.js --all` to add Google data to the remaining unverified restaurants
2. **Clean up duplicates** â€” The database may have duplicates from old name-based matching (é¦™å°é¦†, çœ·æ¹˜, å¤–å©†å®¶å¸¸èœ) â€” use `/review.html` to mark as `duplicate_merged`
3. **Increase scrape yield** â€” current scrape saves ~23-30 posts in 600s; could tune search terms or parallelize
4. **More Bay Area content** â€” tune search terms or expand BAY_AREA_SIGNALS

---

*Written with the perspective of: senior software engineer (architecture), data engineer (pipeline design), frontend engineer (UI/UX scalability).*
