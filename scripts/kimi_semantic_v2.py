#!/usr/bin/env python3
"""
Kimiè¯­ä¹‰æå– v2 - æ”¹è¿›ç‰ˆ
è¿‡æ»¤emojiï¼Œæå–çœŸå®é¤å…å
"""
import json
import re
from pathlib import Path
from collections import defaultdict

def parse_mcp_post(filepath):
    with open(filepath) as f:
        wrapper = json.load(f)
    content_text = wrapper['result']['content'][0]['text']
    data = json.loads(content_text)
    return data['data']['note'], data['data']['comments']['list']

def is_valid_restaurant_name(name):
    """éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é¤å…å"""
    if not name or len(name) < 2:
        return False
    
    # è¿‡æ»¤çº¯emoji
    emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U00002702-\U000027B0\U000024C2-\U0001F251]+')
    if emoji_pattern.fullmatch(name):
        return False
    
    # è¿‡æ»¤å¸¸è§è¡¨æƒ…ç¬¦å·åç§°
    invalid_names = ['R', 'doge', 'è¯·æ–‡æ˜', 'è¯·å‹å¥½', 'ç§ä¿¡', 'èŒèŒå“’', 'è‰²è‰²', 'æ´¾å¯¹', 
                     'ç‚¹èµ', 'æ»¡æœˆ', 'ç¬‘å“­', 'å¾—æ„', 'çš±çœ‰', 'æ‚è„¸', 'å¹æ°”', 'ç©çš„ä»»é€‰']
    if name in invalid_names or any(inv in name for inv in invalid_names):
        return False
    
    # è¿‡æ»¤çº¯è‹±æ–‡çŸ­è¯
    if name.isalpha() and len(name) < 4:
        return False
    
    # è¿‡æ»¤æè¿°æ€§è¯è¯­
    desc_words = ['çš„æ—¶å€™', 'å»è¿‡', 'å‘ç°', 'è¿™å®¶', 'çš„æ—¶å€™', 'å°±å»', 'åƒè¿‡', 'ä¸€æ¬¡', 'ä¹‹å']
    if any(word in name for word in desc_words):
        return False
    
    return True

def kimi_extract_restaurants_v2(title, content):
    """
    Kimiè¯­ä¹‰æå– v2
    æ›´ç²¾ç¡®çš„é¤å…åæå–
    """
    restaurants = []
    
    # 1. ç›´æ¥æ ‡æ³¨çš„é¤å…å ã€XXXã€‘
    pattern1 = r'[ã€\[]([^ã€‘\[\]]{2,20})[ã€‘\]]'
    matches = re.findall(pattern1, content)
    for m in matches:
        if is_valid_restaurant_name(m.strip()):
            restaurants.append({
                'name': m.strip(),
                'confidence': 0.95,
                'method': 'bracket_marker'
            })
    
    # 2. æ¢è¡Œåçš„é¤å…åï¼ˆå¸¸è§äºåˆ—è¡¨ï¼‰
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        # æ¨¡å¼: æ•°å­—. é¤å…å æˆ– - é¤å…å
        match = re.match(r'^(?:\d+[\.\-]\s*|[-â€¢*]\s*)([\u4e00-\u9fa5\w\s]{2,15})(?:\s|$)', line)
        if match:
            name = match.group(1).strip()
            if is_valid_restaurant_name(name):
                restaurants.append({
                    'name': name,
                    'confidence': 0.90,
                    'method': 'list_pattern'
                })
    
    # 3. åŠ¨ä½œå¥å¼ï¼šå»/åœ¨/åƒäº† + é¤å…å
    # å¦‚ "å»äº†äº¬å‘³è½©åƒçƒ¤é¸­"
    pattern3 = r'(?:å»äº†|åœ¨|å»|åˆ°|åƒ|æ¢åº—|æ‰“å¡)\s*([\u4e00-\u9fa5]{2,8}(?:é¤å…|åº—|é¦†|å±‹|å®¶|å°é¦†|é£Ÿå ‚|é¢é¦†|ç²‰åº—|é“ºå­))'
    matches = re.findall(pattern3, content)
    for m in matches:
        if is_valid_restaurant_name(m):
            restaurants.append({
                'name': m,
                'confidence': 0.85,
                'method': 'action_pattern'
            })
    
    # 4. ä»æ ‡é¢˜æå–ï¼ˆå¦‚æœæ ‡é¢˜æ˜ç¡®ï¼‰
    # å¦‚ "Milpitasæ±Ÿå—é›…å¨" - æå– "æ±Ÿå—é›…å¨"
    if '|' in title or 'ï½œ' in title:
        parts = re.split(r'[|ï½œ]', title)
        for part in parts:
            clean = re.sub(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF]+', '', part)
            clean = clean.strip()
            if is_valid_restaurant_name(clean) and len(clean) > 2:
                restaurants.append({
                    'name': clean,
                    'confidence': 0.80,
                    'method': 'title_split'
                })
    
    # å»é‡
    seen = set()
    unique = []
    for r in restaurants:
        if r['name'] not in seen:
            seen.add(r['name'])
            unique.append(r)
    
    return unique

if __name__ == '__main__':
    posts_dir = Path('data/raw/v2/posts/')
    
    all_extracted = defaultdict(lambda: {
        'mentions': [],
        'total_engagement': 0,
        'contexts': []
    })
    
    total_posts = 0
    
    print('ğŸ¤– Kimiè¯­ä¹‰æå– v2 - ä¼˜åŒ–ç‰ˆ')
    print('=' * 70)
    
    for f in sorted(posts_dir.glob('*.json')):
        try:
            note, comments = parse_mcp_post(f)
            total_posts += 1
            
            title = note.get('title', '')
            content = note.get('desc', '')
            engagement = sum([
                int(note.get('interactInfo', {}).get('likedCount', 0) or 0),
                int(note.get('interactInfo', {}).get('commentCount', 0) or 0),
                int(note.get('interactInfo', {}).get('collectedCount', 0) or 0)
            ])
            
            restaurants = kimi_extract_restaurants_v2(title, content)
            
            if restaurants and len([r for r in restaurants if r['confidence'] >= 0.85]) > 0:
                high_conf = [r for r in restaurants if r['confidence'] >= 0.85]
                print(f"\nğŸ“ {title[:45]}")
                for r in high_conf[:3]:
                    print(f"   âœ… {r['name']} ({r['method']})")
            
            for r in restaurants:
                name = r['name']
                all_extracted[name]['mentions'].append({
                    'post_id': note['noteId'],
                    'confidence': r['confidence'],
                    'method': r['method']
                })
                all_extracted[name]['total_engagement'] += engagement
                all_extracted[name]['contexts'].append(content[:100])
                
        except Exception as e:
            pass
    
    print(f'\n{"="*70}')
    print(f'âœ… ä»{total_posts}æ¡å¸–å­ä¸­æå–äº† {len(all_extracted)} å®¶é¤å…å€™é€‰')
    
    # è¿‡æ»¤ä½ç½®ä¿¡åº¦çš„
    valid_restaurants = {
        k: v for k, v in all_extracted.items() 
        if v['total_engagement'] > 50 and len(v['mentions']) >= 1
    }
    
    sorted_restaurants = sorted(
        valid_restaurants.items(),
        key=lambda x: x[1]['total_engagement'],
        reverse=True
    )[:20]
    
    print(f'\nğŸ¯ é«˜è´¨é‡é¤å… Top 20:')
    for i, (name, info) in enumerate(sorted_restaurants, 1):
        mentions = len(info['mentions'])
        engagement = info['total_engagement']
        avg_conf = sum(m['confidence'] for m in info['mentions']) / len(info['mentions'])
        print(f'{i:2d}. {name}')
        print(f'    æåŠ:{mentions}æ¬¡ | äº’åŠ¨:{engagement} | ç½®ä¿¡åº¦:{avg_conf:.2f}')
    
    # ä¿å­˜
    result = {
        'extracted_by': 'Kimi_semantic_v2',
        'total_posts': total_posts,
        'total_candidates': len(all_extracted),
        'valid_restaurants': len(valid_restaurants),
        'restaurants': [
            {
                'name': name,
                'mentions': len(info['mentions']),
                'engagement': info['total_engagement'],
                'avg_confidence': sum(m['confidence'] for m in info['mentions']) / len(info['mentions']),
                'context_sample': info['contexts'][0][:100] if info['contexts'] else ''
            }
            for name, info in sorted_restaurants
        ]
    }
    
    with open('data/extracted_restaurants_kimi_v2.json', 'w') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f'\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° data/extracted_restaurants_kimi_v2.json')
