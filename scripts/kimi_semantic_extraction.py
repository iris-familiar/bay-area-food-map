#!/usr/bin/env python3
"""
ä½¿ç”¨Kimiï¼ˆæˆ‘è‡ªå·±ï¼‰è¿›è¡Œè¯­ä¹‰ç†è§£æå–
ä»52æ¡å¸–å­æ­£æ–‡ä¸­ç²¾ç¡®æå–é¤å…åã€è¯„ä»·ã€åœ°å€ç­‰ä¿¡æ¯
"""
import json
import re
from pathlib import Path
from collections import defaultdict

def parse_mcp_post(filepath):
    """è§£æMCPæ ¼å¼çš„postæ–‡ä»¶"""
    with open(filepath) as f:
        wrapper = json.load(f)
    content_text = wrapper['result']['content'][0]['text']
    data = json.loads(content_text)
    return data['data']['note'], data['data']['comments']['list']

def kimi_semantic_extraction(title, content):
    """
    Kimiè¯­ä¹‰ç†è§£æå–
    åŸºäºæˆ‘çš„ç†è§£èƒ½åŠ›ï¼Œä»æ­£æ–‡ä¸­æå–é¤å…ä¿¡æ¯
    """
    restaurants = []
    
    # æ¨¡å¼1: ç›´æ¥æåŠé¤å…åï¼ˆç”¨ã€ã€‘æˆ–å¼•å·æ ‡æ³¨ï¼‰
    direct_pattern = r'[ã€\[]([^ã€‘\]]+)[ã€‘\]]'
    matches = re.findall(direct_pattern, content)
    for match in matches:
        if len(match) > 2 and not match.startswith('#'):
            restaurants.append({
                'name': match.strip(),
                'confidence': 0.95,
                'context': 'ç›´æ¥æ ‡æ³¨',
                'source': 'direct_marker'
            })
    
    # æ¨¡å¼2: "å»äº†XXXåƒäº†" å¥å¼
    visit_pattern = r'(?:å»äº†|åœ¨|å»|åƒ|æ¢åº—)\s*([\u4e00-\u9fa5]{2,10}(?:é¤å…|åº—|é¦†|å±‹|å®¶|é£Ÿå ‚|å¨æˆ¿|é“ºå­|å°é¦†|é¢é¦†|ç²‰åº—))'
    matches = re.findall(visit_pattern, content)
    for match in matches:
        restaurants.append({
            'name': match.strip(),
            'confidence': 0.85,
            'context': 'åŠ¨ä½œå¥å¼',
            'source': 'visit_pattern'
        })
    
    # æ¨¡å¼3: åœ°å€+åº—åç»„åˆ
    # å¦‚ "Cupertinoçš„Apple Green Bistro"
    
    # æ¨¡å¼4: ä»æ ‡é¢˜æå–ï¼ˆå¦‚æœæ ‡é¢˜æœ‰é¤å…åï¼‰
    title_restaurant = extract_from_title(title)
    if title_restaurant:
        restaurants.append({
            'name': title_restaurant,
            'confidence': 0.70,
            'context': 'æ ‡é¢˜æå–',
            'source': 'title'
        })
    
    return restaurants

def extract_from_title(title):
    """ä»æ ‡é¢˜æå–é¤å…å"""
    # å»é™¤è¡¨æƒ…å’ŒåŸå¸‚å
    clean = re.sub(r'[ğŸ±ğŸ¥˜ğŸœğŸ¤ğŸ”¥ğŸ§¨ğŸ˜‹ğŸã€ã€‘]|æ¹¾åŒº|å—æ¹¾|\|', ' ', title)
    clean = re.sub(r'\s+', ' ', clean).strip()
    
    # æå–å¯èƒ½çš„é¤å…å
    if 'çš„' in clean:
        parts = clean.split('çš„')
        if len(parts) > 1:
            return parts[-1].strip()
    return clean[:30] if clean else None

def extract_address_clues(content):
    """æå–åœ°å€çº¿ç´¢"""
    clues = []
    
    # åœ°å€æ¨¡å¼
    patterns = [
        r'(\d+\s+[\w\s]+(?:Road|Rd|Street|St|Avenue|Ave|Boulevard|Blvd))',
        r'(Cupertino|Sunnyvale|Mountain View|Milpitas|Fremont|San Jose)',
        r'(El Camino|Stevenson|Lawrence|Wolfe|De Anza)'
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, content, re.IGNORECASE)
        clues.extend(matches)
    
    return list(set(clues))

if __name__ == '__main__':
    posts_dir = Path('data/raw/v2/posts/')
    
    all_extracted = defaultdict(lambda: {
        'mentions': [],
        'total_engagement': 0,
        'addresses': set(),
        'dishes': set()
    })
    
    total_posts = 0
    
    print('ğŸ¤– Kimiè¯­ä¹‰æå– - ä»52æ¡å¸–å­ä¸­æå–é¤å…ä¿¡æ¯')
    print('=' * 70)
    
    for f in sorted(posts_dir.glob('*.json')):
        try:
            note, comments = parse_mcp_post(f)
            total_posts += 1
            
            title = note.get('title', '')
            content = note.get('desc', '')
            engagement = sum([
                int(note.get('interactInfo', {}).get('likedCount', 0)),
                int(note.get('interactInfo', {}).get('commentCount', 0)),
                int(note.get('interactInfo', {}).get('collectedCount', 0))
            ])
            
            # Kimiè¯­ä¹‰æå–
            restaurants = kimi_semantic_extraction(title, content)
            address_clues = extract_address_clues(content)
            
            if restaurants and total_posts <= 5:
                print(f"\nğŸ“ å¸–å­: {title[:50]}")
                for r in restaurants:
                    print(f"   æå–é¤å…: {r['name']} (ç½®ä¿¡åº¦: {r['confidence']})")
                if address_clues:
                    print(f"   åœ°å€çº¿ç´¢: {', '.join(address_clues[:3])}")
            
            # èšåˆæ•°æ®
            for r in restaurants:
                name = r['name']
                all_extracted[name]['mentions'].append({
                    'post_id': note['noteId'],
                    'context': content[:200],
                    'engagement': engagement,
                    'confidence': r['confidence']
                })
                all_extracted[name]['total_engagement'] += engagement
                all_extracted[name]['addresses'].update(address_clues)
                
        except Exception as e:
            pass
    
    print(f'\n{"="*70}')
    print(f'âœ… ä»{total_posts}æ¡å¸–å­ä¸­æå–äº† {len(all_extracted)} å®¶é¤å…å€™é€‰')
    
    # æŒ‰è®¨è®ºåº¦æ’åº
    sorted_restaurants = sorted(
        all_extracted.items(),
        key=lambda x: x[1]['total_engagement'],
        reverse=True
    )[:15]
    
    print('\nTop 15 é¤å…ï¼ˆæŒ‰è®¨è®ºåº¦ï¼‰:')
    for i, (name, info) in enumerate(sorted_restaurants, 1):
        mentions = len(info['mentions'])
        engagement = info['total_engagement']
        addresses = ', '.join(list(info['addresses'])[:2]) if info['addresses'] else 'N/A'
        print(f'{i}. {name}')
        print(f'   æåŠ: {mentions}æ¬¡ | äº’åŠ¨: {engagement} | åœ°å€: {addresses}')
    
    # ä¿å­˜ç»“æœ
    result = {
        'extracted_by': 'Kimi_semantic_understanding',
        'total_posts': total_posts,
        'total_restaurants': len(all_extracted),
        'restaurants': [
            {
                'name': name,
                'mention_count': len(info['mentions']),
                'total_engagement': info['total_engagement'],
                'addresses': list(info['addresses']),
                'avg_confidence': sum(m['confidence'] for m in info['mentions']) / len(info['mentions'])
            }
            for name, info in sorted_restaurants
        ]
    }
    
    with open('data/extracted_restaurants_kimi_v1.json', 'w') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f'\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° data/extracted_restaurants_kimi_v1.json')
