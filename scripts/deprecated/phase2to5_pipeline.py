#!/usr/bin/env python3
"""
å®Œæ•´æ•°æ®å¤„ç†Pipeline - Phase 2-5
èšåˆå»é‡ â†’ Google PlaceséªŒè¯ â†’ æ—¶é—´åºåˆ—è®¡ç®— â†’ ç”Ÿæˆæœ€ç»ˆæ•°æ®åº“
"""
import json
import subprocess
import re
from pathlib import Path
from collections import defaultdict
from datetime import datetime, timedelta

# åŠ è½½æå–çš„æ•°æ®
with open('data/extracted_restaurants_kimi_v3.json') as f:
    extracted = json.load(f)

print('='*70)
print('ğŸš€ å®Œæ•´æ•°æ®å¤„ç†Pipeline - Phase 2-5')
print('='*70)

# ============================================
# Phase 2: èšåˆå»é‡ (å·²åœ¨v3è„šæœ¬ä¸­å®Œæˆ)
# ============================================
print('\nğŸ“¦ Phase 2: èšåˆå»é‡')
print(f'   åŸå§‹æå–: {extracted["restaurants_count"]} å®¶é¤å…')
print(f'   æ¥æºå¸–å­: {extracted["total_posts"]} æ¡')

# ============================================
# Phase 3: Google PlaceséªŒè¯
# ============================================
print('\nğŸ” Phase 3: Google PlaceséªŒè¯')

# åŸå¸‚æ˜ å°„è¡¨
city_keywords = {
    'Cupertino': 'Cupertino',
    'Fremont': 'Fremont',
    'Sunnyvale': 'Sunnyvale',
    'Milpitas': 'Milpitas',
    'San Jose': 'San Jose',
    'Mountain View': 'Mountain View',
    'Palo Alto': 'Palo Alto',
    'Santa Clara': 'Santa Clara'
}

def extract_city(contexts):
    """ä»ä¸Šä¸‹æ–‡ä¸­æå–åŸå¸‚"""
    for ctx in contexts:
        ctx_str = str(ctx)
        for city in city_keywords:
            if city in ctx_str:
                return city
    return 'Bay Area'

def verify_with_google_places(name, city):
    """ä½¿ç”¨goplaceséªŒè¯é¤å…"""
    try:
        query = f"{name} {city} CA"
        result = subprocess.run(
            ['goplaces', 'search', query, '--limit', '1', '--json'],
            capture_output=True,
            text=True,
            timeout=10
        )
        if result.returncode == 0 and result.stdout:
            data = json.loads(result.stdout)
            if data.get('results'):
                place = data['results'][0]
                return {
                    'place_id': place.get('place_id', ''),
                    'name': place.get('name', name),
                    'address': place.get('formatted_address', ''),
                    'rating': place.get('rating', 0),
                    'location': place.get('geometry', {}).get('location', {}),
                    'types': place.get('types', []),
                    'verified': True
                }
    except Exception as e:
        print(f'   âš ï¸ éªŒè¯å¤±è´¥ {name}: {e}')
    return None

verified_restaurants = []
for i, r in enumerate(extracted['restaurants'], 1):
    name = r['name']
    city = extract_city(r.get('contexts', []))
    
    print(f'   {i:2d}/{len(extracted["restaurants"])} éªŒè¯: {name} ({city})...', end=' ')
    
    place_info = verify_with_google_places(name, city)
    if place_info:
        print(f'âœ… {place_info.get("rating", 0)}â­')
        verified_restaurants.append({
            'name': name,
            'city': city,
            'google_place_id': place_info.get('place_id', ''),
            'google_name': place_info.get('name', name),
            'address': place_info.get('address', ''),
            'google_rating': place_info.get('rating', 0),
            'location': place_info.get('location', {}),
            'verified': True,
            'raw_data': r
        })
    else:
        print('âŒ æœªæ‰¾åˆ°')
        verified_restaurants.append({
            'name': name,
            'city': city,
            'google_place_id': '',
            'verified': False,
            'raw_data': r
        })

print(f'   âœ… éªŒè¯å®Œæˆ: {sum(1 for r in verified_restaurants if r["verified"])}/{len(verified_restaurants)} å®¶')

# ============================================
# Phase 4: æ—¶é—´åºåˆ—è®¡ç®—
# ============================================
print('\nğŸ“ˆ Phase 4: æ—¶é—´åºåˆ—è®¡ç®—')

# æ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ•°æ®ï¼ˆåŸºäºå½“å‰æ•°æ®æ¨æ–­ï¼‰
# å®é™…åº”è¯¥ä»postsçš„æ—¶é—´æˆ³è®¡ç®—
current_time = datetime.now()

def calculate_timeseries(raw_data):
    """è®¡ç®—æ—¶é—´åºåˆ—æŒ‡æ ‡"""
    engagement = raw_data.get('engagement', {})
    
    # åŸºç¡€äº’åŠ¨æ•°æ®
    total_engagement = (
        engagement.get('posts', 0) * 10 +
        engagement.get('comments', 0) * 2 +
        engagement.get('collected', 0) * 3
    )
    
    # æ¨¡æ‹Ÿè¶‹åŠ¿ï¼ˆåŸºäºäº’åŠ¨æ•°æ¨æ–­ï¼‰
    # å®é™…åº”è¯¥åŸºäºçœŸå®æ—¶é—´æˆ³è®¡ç®—
    trend_7d = min(total_engagement // 5, 100)  # æ¨¡æ‹Ÿ7å¤©è¶‹åŠ¿
    trend_30d = min(total_engagement // 2, 100)  # æ¨¡æ‹Ÿ30å¤©è¶‹åŠ¿
    
    # å£ç¢‘åˆ†æ•°ï¼ˆåŸºäºæ”¶è—/è¯„è®ºæ¯”ä¾‹ï¼‰
    posts = engagement.get('posts', 1)
    collected = engagement.get('collected', 0)
    sentiment_score = min(collected / (posts * 50), 1.0) if posts > 0 else 0.5
    
    return {
        'trend_7d': trend_7d,
        'trend_30d': trend_30d,
        'total_engagement': total_engagement,
        'sentiment_score': round(sentiment_score, 2),
        'mentions': raw_data.get('unique_post_count', 0),
        'comments': raw_data.get('unique_comment_count', 0)
    }

# è®¡ç®—çƒ­é—¨èœå“ï¼ˆä»ä¸Šä¸‹æ–‡æå–ï¼‰
def extract_dishes(contexts):
    """ä»ä¸Šä¸‹æ–‡ä¸­æå–å¯èƒ½çš„èœå“"""
    dishes = []
    dish_keywords = ['çƒ¤é¸­', 'å®«ä¿é¸¡ä¸', 'çƒ¤é±¼', 'é…¸èœé±¼', 'çƒ¤è‚‰', 'æ—©èŒ¶', 
                     'è‚ ç²‰', 'é¥ºå­', 'é¢æ¡', 'ç«é”…', 'çƒ§çƒ¤', 'å¯¿å¸']
    
    for ctx in contexts:
        ctx_str = str(ctx)
        for dish in dish_keywords:
            if dish in ctx_str and dish not in dishes:
                dishes.append(dish)
    
    return dishes[:3] if dishes else ['ç‰¹è‰²èœ']

for r in verified_restaurants:
    raw = r['raw_data']
    r['time_series'] = calculate_timeseries(raw)
    r['popular_dishes'] = extract_dishes(raw.get('contexts', []))
    r['sentiment_score'] = r['time_series']['sentiment_score']

print(f'   âœ… æ—¶é—´åºåˆ—è®¡ç®—å®Œæˆ')

# ============================================
# Phase 5: ç”Ÿæˆæœ€ç»ˆæ•°æ®åº“
# ============================================
print('\nğŸ’¾ Phase 5: ç”Ÿæˆæœ€ç»ˆæ•°æ®åº“')

# ç”Ÿæˆé¤å…ID
final_restaurants = []
for i, r in enumerate(verified_restaurants, 1):
    restaurant = {
        'id': f'r{i:03d}',
        'name': r['name'],
        'google_place_id': r.get('google_place_id', ''),
        'verified': r.get('verified', False),
        'city': r.get('city', 'Bay Area'),
        'address': r.get('address', ''),
        'google_rating': r.get('google_rating', 0),
        'location': r.get('location', {}),
        'time_series': r['time_series'],
        'popular_dishes': r['popular_dishes'],
        'sentiment_score': r['sentiment_score'],
        'mention_contexts': r['raw_data'].get('contexts', [])[:2]
    }
    final_restaurants.append(restaurant)

# æŒ‰äº’åŠ¨æ•°æ’åº
final_restaurants.sort(key=lambda x: x['time_series']['total_engagement'], reverse=True)

# é‡æ–°åˆ†é…ID
for i, r in enumerate(final_restaurants, 1):
    r['id'] = f'r{i:03d}'

database = {
    'version': '5.0',
    'generated_at': datetime.now().isoformat(),
    'total_restaurants': len(final_restaurants),
    'verified_count': sum(1 for r in final_restaurants if r['verified']),
    'restaurants': final_restaurants
}

# ä¿å­˜
data_current = Path('data/current')
data_current.mkdir(parents=True, exist_ok=True)

with open(data_current / 'restaurant_database_v5.json', 'w') as f:
    json.dump(database, f, ensure_ascii=False, indent=2)

print(f'   âœ… æ•°æ®åº“å·²ä¿å­˜: {data_current}/restaurant_database_v5.json')
print(f'   ğŸ“Š æ€»è®¡: {database["total_restaurants"]} å®¶é¤å…')
print(f'   âœ… å·²éªŒè¯: {database["verified_count"]} å®¶')

# æ‰“å°Top 10
print('\n   Top 10 é¤å…:')
for r in final_restaurants[:10]:
    ts = r['time_series']
    verified = 'âœ…' if r['verified'] else 'âŒ'
    print(f'   {r["id"]} {verified} {r["name"]} | '
          f'äº’åŠ¨:{ts["total_engagement"]} | '
          f'å£ç¢‘:{r["sentiment_score"]:.2f} | '
          f'èœå“:{", ".join(r["popular_dishes"])}')

print('\n' + '='*70)
print('âœ… Phase 2-5 å®Œæˆ!')
print('='*70)
