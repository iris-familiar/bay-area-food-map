#!/usr/bin/env python3
"""
æ™ºèƒ½å»é‡ä¸è°ƒåº¦ç³»ç»Ÿ
é˜²æ­¢é‡å¤æŠ“å–ï¼Œæ§åˆ¶æŠ“å–èŠ‚å¥
"""

import json
import hashlib
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Set
import time

class DeduplicationStore:
    """å»é‡å­˜å‚¨ - ä½¿ç”¨SQLiteè®°å½•å·²æŠ“å–å†…å®¹"""
    
    def __init__(self, db_path: str = "data/dedup_store.db"):
        self.db_path = db_path
        self._init_db()
    
    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # å·²æŠ“å–å¸–å­è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS fetched_posts (
                post_id TEXT PRIMARY KEY,
                source_platform TEXT,  -- xiaohongshu, weibo, etc
                content_hash TEXT,      -- å†…å®¹æŒ‡çº¹
                author_id TEXT,
                title TEXT,
                fetch_time TIMESTAMP,
                fetch_job_id TEXT,      -- å“ªæ¬¡æŠ“å–ä»»åŠ¡
                restaurant_mentions TEXT -- æåˆ°çš„é¤å…IDåˆ—è¡¨ï¼ŒJSON
            )
        ''')
        
        # æŠ“å–ä»»åŠ¡æ—¥å¿—
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS fetch_jobs (
                job_id TEXT PRIMARY KEY,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                query TEXT,             -- æœç´¢è¯
                posts_fetched INTEGER,
                posts_new INTEGER,      -- å®é™…æ–°å¢
                posts_duplicate INTEGER, -- é‡å¤æ•°
                status TEXT             -- running, completed, failed
            )
        ''')
        
        # é¤å…è¿½è¸ªè¡¨ - è®°å½•ä¸Šæ¬¡æŠ“å–æ—¶é—´
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS restaurant_tracking (
                restaurant_id TEXT PRIMARY KEY,
                restaurant_name TEXT,
                last_fetch_time TIMESTAMP,
                fetch_count INTEGER DEFAULT 0,
                total_engagement INTEGER,
                priority_score REAL
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def is_duplicate(self, post_id: str, content_hash: str = None) -> bool:
        """æ£€æŸ¥æ˜¯å¦é‡å¤"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ£€æŸ¥post_id
        cursor.execute('SELECT 1 FROM fetched_posts WHERE post_id = ?', (post_id,))
        if cursor.fetchone():
            conn.close()
            return True
        
        # æ£€æŸ¥å†…å®¹hashï¼ˆé˜²æ­¢åŒä¸€ä¸ªå†…å®¹ä¸åŒIDï¼‰
        if content_hash:
            cursor.execute('SELECT 1 FROM fetched_posts WHERE content_hash = ?', (content_hash,))
            if cursor.fetchone():
                conn.close()
                return True
        
        conn.close()
        return False
    
    def add_post(self, post_id: str, source: str, title: str, 
                 author_id: str, restaurants: List[str], job_id: str):
        """æ·»åŠ å·²æŠ“å–è®°å½•"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        content_hash = hashlib.md5(title.encode()).hexdigest()
        
        cursor.execute('''
            INSERT OR REPLACE INTO fetched_posts 
            (post_id, source_platform, content_hash, author_id, title, 
             fetch_time, fetch_job_id, restaurant_mentions)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (post_id, source, content_hash, author_id, title, 
              datetime.now(), job_id, json.dumps(restaurants)))
        
        conn.commit()
        conn.close()
    
    def get_fetch_stats(self, days: int = 7) -> Dict:
        """è·å–æŠ“å–ç»Ÿè®¡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        since = datetime.now() - timedelta(days=days)
        
        cursor.execute('''
            SELECT 
                COUNT(*) as total,
                SUM(posts_new) as new_posts,
                SUM(posts_duplicate) as dup_posts
            FROM fetch_jobs 
            WHERE start_time > ?
        ''', (since,))
        
        row = cursor.fetchone()
        conn.close()
        
        return {
            "total_jobs": row[0] or 0,
            "new_posts": row[1] or 0,
            "duplicate_posts": row[2] or 0,
            "dedup_rate": (row[2] / (row[1] + row[2]) * 100) if (row[1] + row[2]) > 0 else 0
        }


class FetchScheduler:
    """æŠ“å–è°ƒåº¦å™¨ - æ§åˆ¶é¢‘ç‡å’Œè§„æ¨¡"""
    
    # æŠ“å–ç­–ç•¥é…ç½®
    CONFIG = {
        # æ¯æ—¥é™åˆ¶
        "daily_limit": {
            "max_posts": 50,           # æ¯å¤©æœ€å¤šæŠ“50ä¸ªæ–°å¸–å­
            "max_requests": 30,        # æ¯å¤©æœ€å¤š30æ¬¡æœç´¢è¯·æ±‚
            "max_restaurants": 10      # æ¯å¤©æœ€å¤šæ·±åº¦è¿½è¸ª10å®¶é¤å…
        },
        
        # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰- é˜²å°
        "delays": {
            "between_requests": 8,      # è¯·æ±‚é—´éš”8ç§’
            "between_restaurants": 30,  # é¤å…åˆ‡æ¢é—´éš”30ç§’
            "after_error": 60,          # é”™è¯¯åç­‰å¾…60ç§’
            "daily_cooldown": 3600      # è¾¾åˆ°æ—¥é™åˆ¶åå†·å´1å°æ—¶
        },
        
        # å•æ¬¡ä»»åŠ¡è§„æ¨¡
        "batch_size": {
            "posts_per_query": 5,       # æ¯ä¸ªæœç´¢è¯æœ€å¤šå–5ä¸ªå¸–å­
            "queries_per_job": 10,      # æ¯æ¬¡ä»»åŠ¡æœ€å¤š10ä¸ªæœç´¢è¯
            "max_depth_per_restaurant": 3  # æ¯å®¶é¤å…æœ€å¤šæ·±åº¦æœç´¢3å±‚
        },
        
        # é¤å…é‡æŠ“é—´éš”ï¼ˆå¤©ï¼‰
        "refetch_interval": {
            "trending": 3,      # çƒ­é—¨é¤å…3å¤©æ›´æ–°
            "moderate": 7,      # ä¸­ç­‰çƒ­åº¦7å¤©
            "stable": 14,       # ç¨³å®šé¤å…14å¤©
            "insufficient": 1   # æ•°æ®ä¸è¶³1å¤©
        }
    }
    
    def __init__(self, store: DeduplicationStore):
        self.store = store
        self.today_stats = {"posts": 0, "requests": 0, "restaurants": 0}
    
    def can_fetch_today(self, fetch_type: str = "post") -> bool:
        """æ£€æŸ¥ä»Šæ—¥æ˜¯å¦è¿˜å¯ä»¥æŠ“å–"""
        limits = self.CONFIG["daily_limit"]
        
        if fetch_type == "post":
            return self.today_stats["posts"] < limits["max_posts"]
        elif fetch_type == "request":
            return self.today_stats["requests"] < limits["max_requests"]
        elif fetch_type == "restaurant":
            return self.today_stats["restaurants"] < limits["max_restaurants"]
        
        return False
    
    def should_refetch_restaurant(self, restaurant_id: str, 
                                   priority_reason: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡æ–°æŠ“å–æŸé¤å…"""
        conn = sqlite3.connect(self.store.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT last_fetch_time FROM restaurant_tracking 
            WHERE restaurant_id = ?
        ''', (restaurant_id,))
        
        row = cursor.fetchone()
        conn.close()
        
        if not row or not row[0]:
            return True  # ä»æœªæŠ“å–è¿‡
        
        last_fetch = datetime.fromisoformat(row[0])
        days_since = (datetime.now() - last_fetch).days
        
        # æ ¹æ®ä¼˜å…ˆçº§å†³å®šé‡æŠ“é—´éš”
        intervals = self.CONFIG["refetch_interval"]
        required_interval = intervals.get(priority_reason, 7)
        
        return days_since >= required_interval
    
    def get_next_batch(self, candidates: List[Dict]) -> List[Dict]:
        """è·å–ä¸‹ä¸€æ‰¹å¯ä»¥æŠ“å–çš„é¤å…"""
        result = []
        
        for r in candidates:
            # æ£€æŸ¥ä»Šæ—¥é¢åº¦
            if not self.can_fetch_today("restaurant"):
                break
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æŠ“
            if not self.should_refetch_restaurant(r['id'], r.get('priority_reason', 'stable')):
                continue
            
            result.append(r)
            
            # é™åˆ¶æ‰¹æ¬¡å¤§å°
            if len(result) >= self.CONFIG["batch_size"]["queries_per_job"]:
                break
        
        return result
    
    def wait_between_requests(self):
        """è¯·æ±‚é—´ç­‰å¾…"""
        delay = self.CONFIG["delays"]["between_requests"]
        # æ·»åŠ éšæœºæ€§ 8-12ç§’
        import random
        actual_delay = delay + random.randint(0, 4)
        time.sleep(actual_delay)
    
    def record_fetch(self, fetch_type: str, count: int = 1):
        """è®°å½•æœ¬æ¬¡æŠ“å–"""
        if fetch_type in self.today_stats:
            self.today_stats[fetch_type] += count


class AntiDetectionStrategy:
    """åæ£€æµ‹ç­–ç•¥"""
    
    STRATEGIES = {
        # æ—¶é—´æ®µä¼ªè£… - æ¨¡æ‹Ÿæ­£å¸¸ç”¨æˆ·æ´»è·ƒæ—¶é—´
        "time_windows": [
            ("10:00", "12:00"),  # ä¸Šåˆ
            ("14:00", "17:00"),  # ä¸‹åˆ  
            ("19:00", "22:00"),  # æ™šä¸Š
        ],
        
        # éšæœºåŒ–è¯·æ±‚å¤´
        "user_agents": [
            "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X)",
            "Mozilla/5.0 (Linux; Android 14; SM-S918B)",
        ],
        
        # è¡Œä¸ºæ¨¡å¼ - æ¨¡æ‹Ÿäººç±»æµè§ˆ
        "behavior": {
            "scroll_before_fetch": True,    # æŠ“å–å‰æ¨¡æ‹Ÿæ»šåŠ¨
            "random_mouse_move": True,      # éšæœºé¼ æ ‡ç§»åŠ¨
            "view_multiple_posts": True,    # ä¸€æ¬¡çœ‹å¤šä¸ªå¸–å­
        }
    }
    
    @staticmethod
    def is_in_active_hours() -> bool:
        """æ£€æŸ¥æ˜¯å¦åœ¨æ¨¡æ‹Ÿæ´»è·ƒæ—¶é—´æ®µ"""
        from datetime import datetime
        hour = datetime.now().hour
        # é¿å…å‡Œæ™¨å’Œæ·±å¤œæŠ“å–
        return 9 <= hour <= 23
    
    @staticmethod
    def get_random_delay() -> float:
        """è·å–éšæœºå»¶è¿Ÿ"""
        import random
        # æ­£æ€åˆ†å¸ƒï¼Œå‡å€¼8ç§’ï¼Œæ ‡å‡†å·®2ç§’
        delay = random.gauss(8, 2)
        return max(5, min(15, delay))  # é™åˆ¶åœ¨5-15ç§’


class FetchPipeline:
    """å®Œæ•´æŠ“å–æµç¨‹æ§åˆ¶å™¨"""
    
    def __init__(self):
        self.store = DeduplicationStore()
        self.scheduler = FetchScheduler(self.store)
        self.anti_detect = AntiDetectionStrategy()
    
    def generate_fetch_plan(self, search_plan_path: str) -> Dict:
        """ç”Ÿæˆå¯æ‰§è¡Œçš„æŠ“å–è®¡åˆ’"""
        with open(search_plan_path, 'r', encoding='utf-8') as f:
            plan = json.load(f)
        
        executable_plan = {
            "generated_at": datetime.now().isoformat(),
            "total_restaurants": 0,
            "estimated_duration": 0,  # åˆ†é’Ÿ
            "batches": []
        }
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_restaurants = sorted(
            plan['restaurants'],
            key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x.get('priority', 'low')]
        )
        
        # åˆ†æ‰¹å¤„ç†
        batch_size = self.scheduler.CONFIG["batch_size"]["queries_per_job"]
        current_batch = []
        batch_num = 1
        
        for r in sorted_restaurants:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æŠ“
            if not self.scheduler.should_refetch_restaurant(
                r['id'], r.get('priority_reason', 'stable')):
                continue
            
            current_batch.append(r)
            
            if len(current_batch) >= batch_size:
                executable_plan["batches"].append({
                    "batch_id": f"batch_{batch_num}",
                    "restaurants": current_batch,
                    "estimated_time": len(current_batch) * 5  # 5åˆ†é’Ÿæ¯æ‰¹
                })
                batch_num += 1
                current_batch = []
        
        # æ·»åŠ æœ€åä¸€æ‰¹
        if current_batch:
            executable_plan["batches"].append({
                "batch_id": f"batch_{batch_num}",
                "restaurants": current_batch,
                "estimated_time": len(current_batch) * 5
            })
        
        executable_plan["total_restaurants"] = sum(
            len(b["restaurants"]) for b in executable_plan["batches"]
        )
        executable_plan["estimated_duration"] = sum(
            b["estimated_time"] for b in executable_plan["batches"]
        )
        
        return executable_plan
    
    def execute_fetch(self, executable_plan: Dict, dry_run: bool = True):
        """æ‰§è¡ŒæŠ“å–ï¼ˆæˆ–æ¨¡æ‹Ÿï¼‰"""
        print(f"ğŸš€ æŠ“å–è®¡åˆ’æ¦‚è§ˆ")
        print(f"   é¤å…æ€»æ•°: {executable_plan['total_restaurants']}")
        print(f"   åˆ†æ‰¹æ¬¡æ•°: {len(executable_plan['batches'])}")
        print(f"   é¢„è®¡è€—æ—¶: {executable_plan['estimated_duration']} åˆ†é’Ÿ")
        print()
        
        if dry_run:
            print("âš ï¸  è¿™æ˜¯æ¨¡æ‹Ÿè¿è¡Œ (dry-run)")
            print("   å®é™…æ‰§è¡Œè¯·è®¾ç½® dry_run=False")
            return
        
        for batch in executable_plan["batches"]:
            print(f"\nğŸ“¦ æ‰§è¡Œ {batch['batch_id']}")
            
            for restaurant in batch["restaurants"]:
                # æ£€æŸ¥æ´»è·ƒæ—¶é—´
                if not self.anti_detect.is_in_active_hours():
                    print("   â¸ï¸  éæ´»è·ƒæ—¶é—´ï¼Œæš‚åœ...")
                    time.sleep(3600)  # ç­‰å¾…1å°æ—¶
                
                # æ£€æŸ¥ä»Šæ—¥é¢åº¦
                if not self.scheduler.can_fetch_today("restaurant"):
                    print("   ğŸ›‘ ä»Šæ—¥é¢åº¦å·²æ»¡ï¼Œåœæ­¢")
                    return
                
                print(f"   ğŸ” {restaurant['name']}")
                
                # ç­‰å¾…é—´éš”
                self.scheduler.wait_between_requests()
                
                # è¿™é‡Œè°ƒç”¨å®é™…æŠ“å–è„šæœ¬
                # fetch_restaurant_data(restaurant)
                
                # è®°å½•
                self.scheduler.record_fetch("restaurant")
                self.scheduler.record_fetch("requests", len(restaurant['search_queries']))


def main():
    """ä¸»å‡½æ•°"""
    import sys
    
    search_plan = sys.argv[1] if len(sys.argv) > 1 else None
    
    if not search_plan:
        print("Usage: python3 fetch_scheduler.py <search_plan.json>")
        print()
        print("é…ç½®æ–‡ä»¶:")
        print(json.dumps(FetchScheduler.CONFIG, indent=2, ensure_ascii=False))
        return
    
    pipeline = FetchPipeline()
    executable = pipeline.generate_fetch_plan(search_plan)
    
    # ä¿å­˜å¯æ‰§è¡Œè®¡åˆ’
    output_path = search_plan.replace('.json', '_executable.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(executable, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å¯æ‰§è¡Œè®¡åˆ’å·²ä¿å­˜: {output_path}")
    print()
    
    # æ¨¡æ‹Ÿæ‰§è¡Œ
    pipeline.execute_fetch(executable, dry_run=True)


if __name__ == "__main__":
    main()
