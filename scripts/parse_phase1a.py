#!/usr/bin/env python3
"""
å°çº¢ä¹¦æ¹¾åŒºé¤å…æ•°æ®è§£æè„šæœ¬
Phase 1: ä»æœç´¢ç»“æœä¸­æå–é¤å…ä¿¡æ¯
"""

import json
import os
import re
from pathlib import Path

# äºšæ´²èœç³»å…³é”®è¯
ASIAN_CUISINE_KEYWORDS = [
    # ä¸­é¤
    'ä¸­é¤', 'å·èœ', 'æ¹˜èœ', 'ç²¤èœ', 'æ·®æ‰¬èœ', 'æ±Ÿæµ™èœ', 'ä¸Šæµ·èœ', 'åŒ—äº¬èœ', 'ä¸œåŒ—èœ',
    'äº‘å—èœ', 'è´µå·èœ', 'æ–°ç–†èœ', 'è¥¿åŒ—èœ', 'å°æ¹¾èœ', 'é¦™æ¸¯èœ', 'æ—©ç‚¹', 'æ—©èŒ¶', 'ç‚¹å¿ƒ',
    'ç«é”…', 'çƒ§çƒ¤', 'éº»è¾£çƒ«', 'ç ‚é”…', 'åŒ…å­', 'é¢æ¡', 'é¥ºå­', 'é¦„é¥¨', 'ç±³çº¿', 'ç±³ç²‰',
    'æ‹‰é¢', 'ç›–é¢', 'ç…²ä»”é¥­', 'çƒ§è…Š', 'èŒ¶é¤å…', 'æ¹˜', 'å·', 'ç²¤', 'é²', 'è‹', 'æµ™', 'é—½', 'å¾½',
    # æ—¥æ–™
    'æ—¥æ–™', 'æ—¥æœ¬æ–™ç†', 'å¯¿å¸', 'æ‹‰é¢', 'åˆºèº«', 'å±…é…’å±‹', 'çƒ§é¸Ÿ', 'å’Œç‰›',
    # éŸ©é¤
    'éŸ©é¤', 'éŸ©å›½æ–™ç†', 'çƒ¤è‚‰', 'ç‚¸é¸¡', 'éƒ¨é˜Ÿé”…', 'çŸ³é”…æ‹Œé¥­',
    # ä¸œå—äºš
    'æ³°å›½èœ', 'æ³°é¤', 'è¶Šå—èœ', 'è¶Šå—ç²‰', 'æ–°åŠ å¡èœ', 'é©¬æ¥è¥¿äºšèœ', 'å°å°¼èœ', 'ä¸œå—äºš',
    # å…¶ä»–äºšæ´²
    'å°åº¦èœ', 'å°¼æ³Šå°”èœ', 'è’™å¤èœ', 'ç¼…ç”¸èœ', 'è€æŒèœ', 'æŸ¬åŸ”å¯¨èœ',
]

# éäºšæ´²èœç³»ï¼ˆæ’é™¤ï¼‰
NON_ASIAN_CUISINE_KEYWORDS = [
    'æ„å¤§åˆ©', 'æ³•é¤', 'æ³•å›½', 'å¢¨è¥¿å“¥', 'ç¾å¼', 'æ±‰å ¡', 'æŠ«è¨', 'ç‰›æ’', 'è¥¿é¤',
    'è¥¿ç­ç‰™', 'å¾·å›½', 'è‹±å›½', 'å¸Œè…Š', 'åœ°ä¸­æµ·', 'ä¿„ç½—æ–¯', 'å·´è¥¿', 'é˜¿æ ¹å»·',
    'ç§˜é²', 'å¤å·´', 'ç‰™ä¹°åŠ ', 'éæ´²', 'ä¸­ä¸œ', 'åœŸè€³å…¶', 'ä¼Šæœ—', 'é˜¿æ‹‰ä¼¯',
    'è‚¯å¾·åŸº', 'éº¦å½“åŠ³', 'æ˜Ÿå·´å…‹', 'å¿«é¤', 'å¿«é¤åº—'
]

# æ¹¾åŒºåœ°æ ‡éªŒè¯
BAY_AREA_KEYWORDS = [
    'æ¹¾åŒº', 'Cupertino', 'Fremont', 'Milpitas', 'Mountain View', 'Sunnyvale',
    'San Jose', 'Santa Clara', 'Palo Alto', 'Los Altos', 'Saratoga',
    'Campbell', 'Los Gatos', 'Union City', 'Newark', 'Hayward',
    'å—æ¹¾', 'ä¸œæ¹¾', 'åŠå²›', 'ç¡…è°·', 'åŠ å·', 'California', 'CA'
]

def extract_restaurants_from_post(title, display_title=None):
    """ä»å¸–å­æ ‡é¢˜ä¸­æå–é¤å…å€™é€‰"""
    text = display_title if display_title else title
    if not text:
        return []
    
    restaurants = []
    
    # æ¨¡å¼1: XXXé¤å…
    pattern1 = r'([^\s]{2,10})(?:é¤å…|é¤é¦†|é¥­åº—|é£Ÿå ‚|å°é¦†|èœé¦†)'
    matches1 = re.findall(pattern1, text)
    restaurants.extend(matches1)
    
    # æ¨¡å¼2: é¤å…å+|+å…¶ä»–æè¿°
    pattern2 = r'^([^|ã€\s]{2,15})(?:\s*\||\s*ã€|\s*$)'
    matches2 = re.findall(pattern2, text)
    restaurants.extend(matches2)
    
    # æ¨¡å¼3: æ¹¾åŒº XXX
    pattern3 = r'æ¹¾åŒº[\s|ï½œ]+([^ã€\s]{2,15})(?:\s|$|ã€)'
    matches3 = re.findall(pattern3, text)
    restaurants.extend(matches3)
    
    # æ¨¡å¼4: æ¨è/åƒ/å» + é¤å…å
    pattern4 = r'(?:åƒ|å»|æ¢åº—|å°è¯•|å“å°)[äº†è¿‡åˆ°\s]*([^ã€\s]{2,12})(?:\s|$|ã€)'
    matches4 = re.findall(pattern4, text)
    restaurants.extend(matches4)
    
    # æ¸…ç†å¹¶å»é‡
    cleaned = []
    for r in restaurants:
        r = r.strip('ã€ã€‘[]()ï¼ˆï¼‰ï½œ|')
        if len(r) >= 2 and r not in cleaned:
            cleaned.append(r)
    
    return cleaned

def is_asian_restaurant(text):
    """åˆ¤æ–­æ˜¯å¦äºšæ´²é¤å…"""
    if not text:
        return False
    text_lower = text.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«äºšæ´²èœç³»å…³é”®è¯
    for keyword in ASIAN_CUISINE_KEYWORDS:
        if keyword in text:
            return True
    
    # æ£€æŸ¥æ˜¯å¦æ˜ç¡®æ˜¯éäºšæ´²
    for keyword in NON_ASIAN_CUISINE_KEYWORDS:
        if keyword in text:
            return False
    
    return True  # é»˜è®¤ä¿ç•™ï¼Œåç»­äººå·¥å®¡æ ¸

def is_bay_area(text):
    """éªŒè¯æ˜¯å¦åœ¨æ¹¾åŒº"""
    if not text:
        return False
    for keyword in BAY_AREA_KEYWORDS:
        if keyword.lower() in text.lower():
            return True
    return False

def parse_search_result(file_path, city_name):
    """è§£æå•ä¸ªåŸå¸‚æœç´¢ç»“æœ"""
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        return []
    
    # æå–feeds
    try:
        feeds = data['result']['content'][0]['text']
        if isinstance(feeds, str):
            feeds = json.loads(feeds)
        feeds = feeds.get('feeds', [])
    except (KeyError, IndexError, json.JSONDecodeError) as e:
        print(f"Error parsing {file_path}: {e}")
        return []
    
    posts = []
    for feed in feeds:
        if feed.get('modelType') != 'note':
            continue
        
        note_card = feed.get('noteCard', {})
        display_title = note_card.get('displayTitle', '')
        
        # è·³è¿‡æ— æ ‡é¢˜çš„å¸–å­
        if not display_title:
            continue
        
        # äº’åŠ¨æ•°æ®
        interact_info = note_card.get('interactInfo', {})
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯äºšæ´²é¤å…ç›¸å…³å†…å®¹
        if not is_asian_restaurant(display_title):
            continue
        
        # æ£€æŸ¥æ¹¾åŒºç›¸å…³
        if not is_bay_area(display_title):
            continue
        
        post_data = {
            'id': feed.get('id'),
            'xsecToken': feed.get('xsecToken'),
            'title': display_title,
            'city': city_name,
            'likedCount': interact_info.get('likedCount', '0'),
            'sharedCount': interact_info.get('sharedCount', '0'),
            'commentCount': interact_info.get('commentCount', '0'),
            'collectedCount': interact_info.get('collectedCount', '0'),
            'restaurant_candidates': extract_restaurants_from_post(display_title)
        }
        posts.append(post_data)
    
    return posts

def main():
    # æ•°æ®ç›®å½•
    data_dir = Path('/Users/joeli/.openclaw/workspace-planner/projects/bay-area-food-map/data/raw')
    
    # åŸå¸‚æ–‡ä»¶æ˜ å°„
    cities = {
        'cupertino.json': 'Cupertino',
        'fremont.json': 'Fremont',
        'milpitas.json': 'Milpitas',
        'mountain_view.json': 'Mountain View',
        'sunnyvale.json': 'Sunnyvale'
    }
    
    all_posts = []
    city_stats = {}
    
    for filename, city_name in cities.items():
        file_path = data_dir / filename
        if not file_path.exists():
            print(f"File not found: {file_path}")
            continue
        
        posts = parse_search_result(file_path, city_name)
        all_posts.extend(posts)
        city_stats[city_name] = len(posts)
        print(f"âœ… {city_name}: {len(posts)} æ¡æœ‰æ•ˆå¸–å­")
    
    # æ±‡æ€»ç»Ÿè®¡
    print(f"\nğŸ“Š Phase 1A æ±‡æ€»:")
    print(f"- æ€»å¸–å­æ•°: {len(all_posts)}")
    for city, count in city_stats.items():
        print(f"  - {city}: {count} æ¡")
    
    # æå–æ‰€æœ‰é¤å…å€™é€‰
    all_restaurants = {}
    for post in all_posts:
        for restaurant in post['restaurant_candidates']:
            if restaurant not in all_restaurants:
                all_restaurants[restaurant] = {
                    'name': restaurant,
                    'mentions': [],
                    'cities': set()
                }
            all_restaurants[restaurant]['mentions'].append({
                'post_id': post['id'],
                'title': post['title'],
                'city': post['city'],
                'likedCount': post['likedCount']
            })
            all_restaurants[restaurant]['cities'].add(post['city'])
    
    # è½¬æ¢ä¸ºåˆ—è¡¨å¹¶æ’åºï¼ˆæŒ‰æåŠæ¬¡æ•°ï¼‰
    restaurant_list = []
    for name, data in all_restaurants.items():
        restaurant_list.append({
            'name': name,
            'mentions_count': len(data['mentions']),
            'cities': list(data['cities']),
            'mentions': data['mentions']
        })
    
    restaurant_list.sort(key=lambda x: x['mentions_count'], reverse=True)
    
    print(f"\nğŸ´ å‘ç°é¤å…å€™é€‰: {len(restaurant_list)} å®¶")
    print("\nTop 20 é¤å…å€™é€‰:")
    for i, r in enumerate(restaurant_list[:20], 1):
        print(f"  {i}. {r['name']} (æåŠ{r['mentions_count']}æ¬¡, åŸå¸‚: {', '.join(r['cities'])})")
    
    # ä¿å­˜ç»“æœ
    output = {
        'phase': '1A',
        'search_date': '2026-02-15',
        'total_posts': len(all_posts),
        'total_restaurant_candidates': len(restaurant_list),
        'city_stats': city_stats,
        'posts': all_posts,
        'restaurant_candidates': restaurant_list
    }
    
    output_file = data_dir / 'phase1a_search_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜: {output_file}")
    
    return output

if __name__ == '__main__':
    main()
