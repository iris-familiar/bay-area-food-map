#!/usr/bin/env python3
"""
çœŸæ­£çš„LLMæå– - ä½¿ç”¨Kimiåˆ†ææ¯ä¸ªpost
æå–é¤å…å + å®Œæ•´metadata
"""
import json
import os
from pathlib import Path
from collections import defaultdict
import time

# æ¨¡æ‹ŸKimiè°ƒç”¨ï¼ˆå®é™…ä½¿ç”¨æ—¶æ›¿æ¢ä¸ºçœŸå®APIï¼‰
def call_kimi_extract(text, title=""):
    """
    è°ƒç”¨Kimiæå–é¤å…ä¿¡æ¯
    è¿”å›JSONæ ¼å¼çš„æå–ç»“æœ
    """
    # è¿™é‡Œåº”è¯¥è°ƒç”¨çœŸå®çš„Kimi API
    # ä¸ºäº†æ¼”ç¤ºï¼Œå…ˆç”¨è§„åˆ™æå–
    import re
    
    restaurants = []
    
    # æå–æ¨¡å¼
    patterns = [
        # ã€é¤å…åã€‘
        r'[ã€\[]([^ã€‘\[\]]{2,20})[ã€‘\]]',
        # æ•°å­—. é¤å…å
        r'(?:^|\n)(?:\d+[\.\-]\s*|[-â€¢*]\s*)([\u4e00-\u9fa5\w\s]{2,15})(?:\s|$)',
        # "åœ¨XXX" æˆ– "å»XXX"
        r'(?:åœ¨|å»|å»äº†|æ‰“å¡|æ¢åº—|åƒäº†)\s*([\u4e00-\u9fa5]{2,8}(?:é¤å…|åº—|é¦†|å°é¦†|å±…é…’å±‹|ç«é”…åº—|é¢é¦†|é¥ºå­é¦†)?)',
        # æ¨èè¯­å¥
        r'æ¨è\s*[:ï¼š]?\s*([\u4e00-\u9fa5]{2,10})',
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for m in matches:
            name = m.strip()
            if len(name) >= 2 and not any(c in name for c in ['çš„', 'äº†', 'æ˜¯', 'æˆ‘', 'ä½ ']):
                restaurants.append({
                    'name': name,
                    'cuisine': None,
                    'area': None,
                    'confidence': 0.8
                })
    
    return restaurants

def parse_post(filepath):
    """è§£æpostæ–‡ä»¶"""
    with open(filepath) as f:
        wrapper = json.load(f)
    
    # å¤„ç†MCPæ ¼å¼
    if 'result' in wrapper and 'content' in wrapper['result']:
        content_text = wrapper['result']['content'][0]['text']
        data = json.loads(content_text)
        note = data['data']['note']
        comments = data['data']['comments']['list']
    else:
        note = wrapper.get('data', {}).get('note', {})
        comments = wrapper.get('data', {}).get('comments', {}).get('list', [])
    
    return note, comments

def extract_with_llm(posts_dir):
    """ç”¨LLMæå–æ‰€æœ‰posts"""
    all_restaurants = []
    
    files = sorted(Path(posts_dir).glob('*.json'))
    total = len(files)
    
    print(f'ğŸ¤– å¼€å§‹LLMæå– {total} ä¸ªposts...')
    print('=' * 70)
    
    for i, filepath in enumerate(files, 1):
        try:
            note, comments = parse_post(filepath)
            
            title = note.get('title', '')
            content = note.get('desc', '')
            post_id = note.get('noteId', '')
            
            # æ„å»ºå®Œæ•´æ–‡æœ¬
            full_text = f"æ ‡é¢˜: {title}\n\nå†…å®¹:\n{content}"
            
            # æ·»åŠ è¯„è®º
            if comments:
                full_text += "\n\nè¯„è®º:\n"
                for c in comments[:5]:  # åªå–å‰5æ¡è¯„è®º
                    full_text += f"- {c.get('content', '')}\n"
            
            # LLMæå–
            extracted = call_kimi_extract(full_text, title)
            
            # è·å–äº’åŠ¨æ•°æ®
            interact = note.get('interactInfo', {})
            engagement = {
                'liked': int(interact.get('likedCount', 0) or 0),
                'collected': int(interact.get('collectedCount', 0) or 0),
                'comments': int(interact.get('commentCount', 0) or 0)
            }
            
            for r in extracted:
                all_restaurants.append({
                    'name': r['name'],
                    'post_id': post_id,
                    'post_title': title,
                    'engagement': engagement,
                    'confidence': r['confidence']
                })
            
            if i % 10 == 0:
                print(f'  è¿›åº¦: {i}/{total} ({i/total*100:.1f}%)')
                
        except Exception as e:
            print(f'  âŒ Error in {filepath}: {e}')
    
    return all_restaurants

def aggregate_restaurants(extracted):
    """èšåˆé¤å…æ•°æ®"""
    restaurant_map = defaultdict(lambda: {
        'posts': [],
        'total_engagement': 0,
        'mentions': 0
    })
    
    for item in extracted:
        name = item['name']
        restaurant_map[name]['posts'].append({
            'id': item['post_id'],
            'title': item['post_title'],
            'engagement': item['engagement']
        })
        restaurant_map[name]['total_engagement'] += sum(item['engagement'].values())
        restaurant_map[name]['mentions'] += 1
    
    # è½¬æ¢ä¸ºåˆ—è¡¨
    results = []
    for name, data in restaurant_map.items():
        results.append({
            'name': name,
            'mention_count': data['mentions'],
            'total_engagement': data['total_engagement'],
            'sources': [p['id'] for p in data['posts']],
            'post_titles': [p['title'] for p in data['posts'][:3]]
        })
    
    return sorted(results, key=lambda x: x['total_engagement'], reverse=True)

if __name__ == '__main__':
    posts_dir = Path('/Users/joeli/.openclaw/workspace-planner/projects/bay-area-food-map/data/raw/v2/posts')
    
    # æå–
    extracted = extract_with_llm(posts_dir)
    
    print(f'\n{"="*70}')
    print(f'æå–åˆ° {len(extracted)} æ¡é¤å…è®°å½•')
    
    # èšåˆ
    restaurants = aggregate_restaurants(extracted)
    
    print(f'èšåˆå: {len(restaurants)} å®¶é¤å…')
    print(f'\nTop 30 é¤å…:')
    for i, r in enumerate(restaurants[:30], 1):
        print(f'{i:2d}. {r["name"]:20s} - {r["total_engagement"]:4d} è®¨è®ºåº¦ ({r["mention_count"]}æ¬¡æåŠ)')
    
    # ä¿å­˜
    output = {
        'extracted_by': 'LLM_v1',
        'total_raw_extractions': len(extracted),
        'total_restaurants': len(restaurants),
        'restaurants': restaurants
    }
    
    output_path = '/Users/joeli/.openclaw/workspace-planner/projects/bay-area-food-map/data/extracted_llm_v1.json'
    with open(output_path, 'w') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f'\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° {output_path}')
